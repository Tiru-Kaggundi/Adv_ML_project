{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoS Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOAycoxJMQza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ce50d7-a737-4e94-bf70-ff6d3c6aedda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.3.5)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Installing collected packages: urllib3, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed torchdata-0.3.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install indic-nlp-library\n",
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install spacy\n",
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "zlV84VFRNjxZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5HWq3XoaAPi",
        "outputId": "a7e14d14-e682-46c1-ed8d-b0ca01c594f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd /content/gdrive/MyDrive/AML_Project/\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/AML_Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjeGG3VnM5N7",
        "outputId": "28b1cbd4-fcc2-4398-f238-eadc8754d19d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/AML_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df = pd.DataFrame(columns=['sentence'])\n",
        "with open('/content/gdrive/MyDrive/AML_Project/tagger_data/newTrainH1.txt',encoding=\"utf8\") as fp:\n",
        "   line = fp.readline()\n",
        "   cnt = 0\n",
        "   while line:\n",
        "       #print(\"Line {}: {}\".format(cnt, line.strip()))\n",
        "       line = fp.readline()\n",
        "       line = line.replace('_', '')\n",
        "       df.loc[cnt, ['sentence']] = line\n",
        "       cnt += 1 \n",
        "       \n",
        "\n",
        "def split_for_tagger(sentence):\n",
        "    res = re.findall(r'\\[.*?\\]', sentence)\n",
        "    RES= []\n",
        "    for elem in res:\n",
        "        #print(elem)\n",
        "        a = elem.replace('[','').replace(']','')\n",
        "        b = a.replace(\" \",\"\")\n",
        "        RES.append(b)\n",
        "    return RES\n",
        "\n",
        "def split_for_word(sentence):\n",
        "    string = re.sub(\"\\[.*?\\]\",\"\",sentence)\n",
        "    c= string.split()\n",
        "    return c\n",
        "    \n",
        "df['pos'] = df['sentence'].apply(split_for_tagger)\n",
        "df['clean_sentence'] = df['sentence'].apply(split_for_word)\n",
        "    \n",
        "df.drop(['sentence'], axis=1, inplace= True)\n"
      ],
      "metadata": {
        "id": "MglhH4rZMj7v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "896U7nksNVH2",
        "outputId": "0d251109-383a-4f03-c4a4-74dbad79ad3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    pos  \\\n",
              "0     [JJ, NN, PSP, NN, SYM, PRP, NN, SYM, NN, PSP, ...   \n",
              "1     [JJ, SYM, JJ, SYM, NST, VM, PSP, JJ, NN, PSP, ...   \n",
              "2     [PRP, NN, PSP, INTF, JJ, NN, PRP, NN, VM, VAUX...   \n",
              "3     [PRP, NN, VM, CC, CC, PRP, SYM, NN, SYM, CC, S...   \n",
              "4     [CC, PRP, PRP, PRP, NN, PSP, NN, VM, VAUX, VAU...   \n",
              "...                                                 ...   \n",
              "2595  [NNP, PSP, VM, SYM, SYM, NNP, CC, NNP, JJ, JJ,...   \n",
              "2596  [PRP, JJ, NN, PSP, QO, NN, NN, NNP, PSP, NNP, ...   \n",
              "2597  [NNP, PSP, DEM, NNP, VM, PRP, DEM, NN, NNP, NN...   \n",
              "2598  [NN, PSP, JJ, NN, NNP, PSP, NNP, NNP, NNP, VM,...   \n",
              "2599                                                 []   \n",
              "\n",
              "                                         clean_sentence  \n",
              "0     [पुरानी, पीढ़ी, के, भारतीय, --, हमारे, पिता-पित...  \n",
              "1     [बदमिज़ाज, ,, चिड़चिड़े, ,, दूर-दूर, रहने, वाले, ...  \n",
              "2     [अपनी, ऐंठ, में, थोड़ा-बहुत, भद्र, पीढ़ी-दर-पीढ़ी...  \n",
              "3     [मुझे, विश्वास, है, कि, अगर, उनकी, `, खुशमिज़ाज...  \n",
              "4     [और, जब, मैं, अपने, पिता, की, बात, कर, रहा, हू...  \n",
              "...                                                 ...  \n",
              "2595  [शिराक़, ने, कहा, ,, \", यूरोप, और, अमरीका, सच्...  \n",
              "2596  [अपने, यूरोपीय, दौरे, के, दूसरे, दिन, राष्ट्रप...  \n",
              "2597  [बुधवार, को, वे, जर्मनी, जाएँगे, जहाँ, वे, चां...  \n",
              "2598  [दौरे, के, अंतिम, दिन, गुरूवार, को, जॉर्ज, बुश...  \n",
              "2599                                                 []  \n",
              "\n",
              "[2600 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9395aba-ed5f-4670-8700-d8e5730387d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[JJ, NN, PSP, NN, SYM, PRP, NN, SYM, NN, PSP, ...</td>\n",
              "      <td>[पुरानी, पीढ़ी, के, भारतीय, --, हमारे, पिता-पित...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[JJ, SYM, JJ, SYM, NST, VM, PSP, JJ, NN, PSP, ...</td>\n",
              "      <td>[बदमिज़ाज, ,, चिड़चिड़े, ,, दूर-दूर, रहने, वाले, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[PRP, NN, PSP, INTF, JJ, NN, PRP, NN, VM, VAUX...</td>\n",
              "      <td>[अपनी, ऐंठ, में, थोड़ा-बहुत, भद्र, पीढ़ी-दर-पीढ़ी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[PRP, NN, VM, CC, CC, PRP, SYM, NN, SYM, CC, S...</td>\n",
              "      <td>[मुझे, विश्वास, है, कि, अगर, उनकी, `, खुशमिज़ाज...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CC, PRP, PRP, PRP, NN, PSP, NN, VM, VAUX, VAU...</td>\n",
              "      <td>[और, जब, मैं, अपने, पिता, की, बात, कर, रहा, हू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2595</th>\n",
              "      <td>[NNP, PSP, VM, SYM, SYM, NNP, CC, NNP, JJ, JJ,...</td>\n",
              "      <td>[शिराक़, ने, कहा, ,, \", यूरोप, और, अमरीका, सच्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2596</th>\n",
              "      <td>[PRP, JJ, NN, PSP, QO, NN, NN, NNP, PSP, NNP, ...</td>\n",
              "      <td>[अपने, यूरोपीय, दौरे, के, दूसरे, दिन, राष्ट्रप...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2597</th>\n",
              "      <td>[NNP, PSP, DEM, NNP, VM, PRP, DEM, NN, NNP, NN...</td>\n",
              "      <td>[बुधवार, को, वे, जर्मनी, जाएँगे, जहाँ, वे, चां...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2598</th>\n",
              "      <td>[NN, PSP, JJ, NN, NNP, PSP, NNP, NNP, NNP, VM,...</td>\n",
              "      <td>[दौरे, के, अंतिम, दिन, गुरूवार, को, जॉर्ज, बुश...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2599</th>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2600 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9395aba-ed5f-4670-8700-d8e5730387d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9395aba-ed5f-4670-8700-d8e5730387d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9395aba-ed5f-4670-8700-d8e5730387d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"example\"] = df.apply(lambda row: [row.clean_sentence, row.pos], axis=1)"
      ],
      "metadata": {
        "id": "VvfPTFROOAH_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HINDIPOS_ALL = df[[\"example\"]]\n",
        "HINDIPOS = HINDIPOS_ALL.loc[0:2999]\n",
        "HINDIPOS_test = HINDIPOS_ALL.loc[2300:2598]\n",
        "HINDIPOS_test = HINDIPOS_test.reset_index()[[\"example\"]]\n"
      ],
      "metadata": {
        "id": "uPJn1niuOUdX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S7Z2DkvDZNvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# create custom dataset class\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.examples = df.iloc[:,0]\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.__len__():\n",
        "          raise StopIteration\n",
        "        example = self.examples[idx]\n",
        "        return example"
      ],
      "metadata": {
        "id": "okjqIw2pOjVb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doing Preprocessing"
      ],
      "metadata": {
        "id": "FibRKQIAZXRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = 1\n",
        "WINDOW_SIZE = (2 * W + 1)\n",
        "\n",
        "SENT_START_WORD = '<s>'\n",
        "SENT_END_WORD = '</s>'\n",
        "SENT_START_TAG = '<STAG>'\n",
        "SENT_END_TAG = '<ETAG>'\n",
        "\n",
        "\n",
        "\n",
        "def add_sent_start_end(data_iter, w):\n",
        "    for (words, ud_tags) in data_iter:\n",
        "        new_words = [SENT_START_WORD] * w + words + [SENT_END_WORD] * w\n",
        "        new_ud_tags = [SENT_START_TAG] * w + ud_tags + [SENT_END_TAG] * w\n",
        "        yield(new_words, new_ud_tags)\n",
        "        \n",
        "def create_windows(data_iter, w):\n",
        "    window_size = 2*w + 1\n",
        "    for (words, ud_tags) in data_iter:\n",
        "        words_zip = zip(*[words[i:] for i in range(window_size)])\n",
        "        ud_zip = zip(*[ud_tags[i:] for i in range(window_size)])\n",
        "        ## ADD YOUR CODE ABOVE\n",
        "        for word_sseq, ud_sseq in zip(\n",
        "                words_zip, ud_zip):\n",
        "            yield(word_sseq, ud_sseq)\n",
        "            \n",
        "def preprocess_data_seq(data_iter, w):\n",
        "    ## MISSING PART: ADD YOUR CODE BELOW\n",
        "    data_iter_new = add_sent_start_end(data_iter, w)\n",
        "    rv = create_windows(data_iter_new, w) \n",
        "    return rv"
      ],
      "metadata": {
        "id": "YonIlFzrOq8M"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating vocabularies for words and tags"
      ],
      "metadata": {
        "id": "R13sSGJ-Zmv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter_0 = CustomTextDataset(HINDIPOS)\n",
        "test_iter_0 = CustomTextDataset(HINDIPOS_test)\n",
        "train_iter_vocab = preprocess_data_seq(train_iter_0, 1)\n",
        "\n",
        "counter_words = Counter()\n",
        "counter_ud = Counter()\n",
        "for (text, pos_ud) in train_iter_vocab:\n",
        "    counter_words.update(text)\n",
        "    counter_ud.update(pos_ud)\n",
        "vocab_words = vocab(counter_words,  specials = ['<unk>'], special_first = True)    \n",
        "vocab_words.set_default_index(0)\n",
        "vocab_ud = vocab(counter_ud)\n",
        "vocab_ud.set_default_index(-1)"
      ],
      "metadata": {
        "id": "EqobKjjRO3eo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TAG = 'ud'\n",
        "def collate_fn(batch, w = W, tag = TAG):\n",
        "    word_idxs= []\n",
        "    labels= []\n",
        "    ## WRITE YOUR CODE BELOW\n",
        "    for ind in range (len(batch)):\n",
        "      word_list= batch[ind][0]\n",
        "      word_idx= []\n",
        "      for word in word_list:\n",
        "        idx= vocab_words[word]\n",
        "        word_idx.append(idx)\n",
        "      center= w\n",
        "      pos= batch[ind][1][center]\n",
        "      pos_idx= vocab_ud[pos]\n",
        "      labels.append(pos_idx)\n",
        "      word_idxs.append(word_idx)\n",
        "    word_idxs= torch.tensor(word_idxs)\n",
        "    labels= torch.tensor(labels)\n",
        "    ## WRITE YOUR CODE ABOVE\n",
        "    # The tensors you return should be placed in the correct device\n",
        "    # as shown below.\n",
        "    return labels.to(device), word_idxs.to(device)"
      ],
      "metadata": {
        "id": "3z4r6Y3CPQzd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Model"
      ],
      "metadata": {
        "id": "xllDTMyrbJ4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NNPOSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 window_size,\n",
        "                 vocab_size, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 nonlinearity, \n",
        "                 # These are used for later tasks\n",
        "                 use_glove = False, \n",
        "                 freeze_glove = False):      \n",
        "        super(NNPOSTagger, self).__init__()\n",
        "        self.window_size= window_size\n",
        "        self.vocab_size= vocab_size\n",
        "        self.embedding_dim= embedding_dim\n",
        "        self.hidden_dim= hidden_dim\n",
        "        self.output_dim= output_dim\n",
        "        self.nonlinearity= nonlinearity\n",
        "\n",
        "        #self.layer1 = nn.linear((2*w+1)*300, 128)\n",
        "        self.layer1 = nn.Linear(window_size*embedding_dim, hidden_dim)\n",
        "\n",
        "        #self.layer2 = nn.linear(128, len(vocab_ud))\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "      \n",
        "        \n",
        "    def forward(self, word_idxs_batch):\n",
        "        \n",
        "        ## WRITE YOUR CODE BELOW.\n",
        "\n",
        "        embedded = self.embedding(word_idxs_batch)\n",
        "        embedded_final = embedded.view(len(word_idxs_batch), 900) # transforming 3 x 300 to 1 x 900\n",
        "\n",
        "        output1 = self.layer1(embedded_final) # will be fed into to get h\n",
        "        h = self.nonlinearity(output1) # calculating h\n",
        "        y_tilda = self.layer2(h) #calculating  y-tilde\n",
        "\n",
        "        y_hat= nn.functional.log_softmax(y_tilda, dim=1)\n",
        "\n",
        "        return y_hat # this should be of output dimension shape"
      ],
      "metadata": {
        "id": "xs9LxJ40PQ_B"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NNPOSTagger(window_size = WINDOW_SIZE, \n",
        "                    vocab_size = len(vocab_words), \n",
        "                     embedding_dim = 300, \n",
        "                     hidden_dim = 128, \n",
        "                     output_dim = len(vocab_ud),\n",
        "                     nonlinearity = nn.Tanh(), \n",
        "                     use_glove = False,\n",
        "                     freeze_glove = False).to(device)\n",
        "                "
      ],
      "metadata": {
        "id": "RP6L_16HPRHy"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training an epoch"
      ],
      "metadata": {
        "id": "hEMHxhbDbbKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.NLLLoss()\n",
        "\n",
        "def train_an_epoch(dataloader):\n",
        "    \n",
        "    model.train() # Sets the module in training mode.\n",
        "    log_interval = 500\n",
        "    \n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        model.zero_grad()\n",
        "        log_probs = model(text)\n",
        "        loss = loss_function(log_probs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(f'At iteration {idx} the loss is {loss:.3f}.')\n",
        "    "
      ],
      "metadata": {
        "id": "v8ShYfPvbQ_j"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get accuracy"
      ],
      "metadata": {
        "id": "_pJ9svBFcT1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():    \n",
        "        total_acc, total_count = 0, 0\n",
        "        for idx, (label, word_idxs) in enumerate(dataloader):\n",
        "            log_probs = model(word_idxs)\n",
        "            total_acc += (log_probs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "2loLYqvHbeHa"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 # batch size for training\n",
        "train_data = list(preprocess_data_seq(train_iter_0, W))\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, \n",
        "                              collate_fn=collate_fn)\n",
        "\n",
        "test_data = list(preprocess_data_seq(test_iter_0, W))\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, \n",
        "                             collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "CSRGS_gze80X"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "EPOCHS = 20 # epoch\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "accuracies=[]\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_an_epoch(train_dataloader)\n",
        "    accuracy = get_accuracy(test_dataloader)\n",
        "    accuracies.append(accuracy)\n",
        "    time_taken = time.time() - epoch_start_time\n",
        "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
        "    \n",
        "plt.plot(range(1, EPOCHS+1), accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "mOlBS7D8j5vm",
        "outputId": "2536ed78-d190-4e39-beeb-90b67098a8e5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At iteration 500 the loss is 1.150.\n",
            "Epoch: 1, time taken: 17.4s, validation accuracy: 0.688.\n",
            "At iteration 500 the loss is 0.903.\n",
            "Epoch: 2, time taken: 16.5s, validation accuracy: 0.746.\n",
            "At iteration 500 the loss is 0.724.\n",
            "Epoch: 3, time taken: 16.6s, validation accuracy: 0.782.\n",
            "At iteration 500 the loss is 0.446.\n",
            "Epoch: 4, time taken: 16.7s, validation accuracy: 0.816.\n",
            "At iteration 500 the loss is 0.620.\n",
            "Epoch: 5, time taken: 17.6s, validation accuracy: 0.844.\n",
            "At iteration 500 the loss is 0.368.\n",
            "Epoch: 6, time taken: 16.5s, validation accuracy: 0.863.\n",
            "At iteration 500 the loss is 0.280.\n",
            "Epoch: 7, time taken: 16.4s, validation accuracy: 0.873.\n",
            "At iteration 500 the loss is 0.535.\n",
            "Epoch: 8, time taken: 16.6s, validation accuracy: 0.889.\n",
            "At iteration 500 the loss is 0.302.\n",
            "Epoch: 9, time taken: 16.5s, validation accuracy: 0.904.\n",
            "At iteration 500 the loss is 0.116.\n",
            "Epoch: 10, time taken: 16.4s, validation accuracy: 0.916.\n",
            "At iteration 500 the loss is 0.495.\n",
            "Epoch: 11, time taken: 16.5s, validation accuracy: 0.927.\n",
            "At iteration 500 the loss is 0.254.\n",
            "Epoch: 12, time taken: 16.6s, validation accuracy: 0.935.\n",
            "At iteration 500 the loss is 0.101.\n",
            "Epoch: 13, time taken: 16.4s, validation accuracy: 0.941.\n",
            "At iteration 500 the loss is 0.223.\n",
            "Epoch: 14, time taken: 16.4s, validation accuracy: 0.950.\n",
            "At iteration 500 the loss is 0.304.\n",
            "Epoch: 15, time taken: 16.5s, validation accuracy: 0.957.\n",
            "At iteration 500 the loss is 0.276.\n",
            "Epoch: 16, time taken: 16.5s, validation accuracy: 0.964.\n",
            "At iteration 500 the loss is 0.164.\n",
            "Epoch: 17, time taken: 16.4s, validation accuracy: 0.972.\n",
            "At iteration 500 the loss is 0.070.\n",
            "Epoch: 18, time taken: 16.5s, validation accuracy: 0.977.\n",
            "At iteration 500 the loss is 0.122.\n",
            "Epoch: 19, time taken: 16.4s, validation accuracy: 0.979.\n",
            "At iteration 500 the loss is 0.037.\n",
            "Epoch: 20, time taken: 16.3s, validation accuracy: 0.985.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe2ef4cf410>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}