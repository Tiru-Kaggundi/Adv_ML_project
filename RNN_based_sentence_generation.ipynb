{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_based_sentence_generation.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA8JWuK07xUeR+722/jY+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiru-Kaggundi/Adv_ML_project/blob/main/RNN_based_sentence_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paragraph generation using Transformer models:\n",
        "\n",
        "We will attempt to generate a paragraph of text in Hindi language based on a given prompt. We would attempt to train the model based on available online speech of the Prime Minister of India (Mr Narendra Modi) and examine if the prompt is able to generate a paragraph that matches the style closely. We would also examine if the approach can be extended to some famous authors in Hindi language such as Shri Munshi Premchand.\n"
      ],
      "metadata": {
        "id": "h267_xzWgoQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "id": "pfV4WmEc5hx_",
        "outputId": "7e72e524-3401-4465-e6e5-0ff34e8ff60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.11.2\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Collecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x3a436000 @  0x7f885593a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2022.5.18.1)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On Colab, you'll see ('1.10.2+cu102', '0.11.2')\n",
        "import torch, torchtext\n",
        "torch.__version__, torchtext.__version__"
      ],
      "metadata": {
        "id": "VmXOQMwr5tLY",
        "outputId": "2bcd188c-d396-4eff-c24b-7d77712cd69f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.10.2+cu102', '0.11.2')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import Vectors\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using cuda.\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using cpu.\")\n",
        "\n",
        "random.seed(30255)\n",
        "np.random.seed(30255)\n",
        "torch.manual_seed(30255)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(30255)\n",
        "\n",
        "# Change the following to false when training on\n",
        "# the full set\n",
        "DEVELOPING = False    \n",
        "#DEVELOPING = True\n",
        "\n",
        "if DEVELOPING:\n",
        "    print('Small development version')\n",
        "    BATCH_SIZE = 4\n",
        "    EMBEDDING_SIZE = 20\n",
        "    MAX_VOCAB_SIZE = 5000\n",
        "    TRAIN_DATA_SET = \"lm-train-small.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev-small.txt\"\n",
        "    TEST_DATA_SET = \"lm-test-small.txt\"\n",
        "    BPTT_LENGTH = 8\n",
        "else:\n",
        "    print('Full version')\n",
        "    BATCH_SIZE = 32\n",
        "    EMBEDDING_SIZE = 650\n",
        "    MAX_VOCAB_SIZE = 50000\n",
        "    TRAIN_DATA_SET = \"lm-train.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev.txt\"\n",
        "    TEST_DATA_SET = \"lm-test.txt\"\n",
        "    BPTT_LENGTH = 32\n",
        "\n"
      ],
      "metadata": {
        "id": "YbwLnZ1S6N7D",
        "outputId": "32561723-df49-40bc-8802-db36cdd193dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n",
            "Full version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For uploading data to Colab see, e.g., \n",
        "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
        "# COLAB = False\n",
        "COLAB = True\n",
        "if COLAB:\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')\n",
        "    PATH = \"gdrive/My Drive/AML_Project/datasets_hindi_wiki\"\n",
        "else:\n",
        "    PATH = \"/Users/tiru/Documents/Documents_new/Adv_ML/hindi_wiki_55k\"\n",
        "    \n",
        "LOG_FILE = \"language-model.log\""
      ],
      "metadata": {
        "id": "9_4nob7dGoS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This command is to unmount gdrive - need it sometimes\n",
        "#!fusermount -u gdrive"
      ],
      "metadata": {
        "id": "AX6BNsPo6yb5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True)\n",
        "\n",
        "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \n",
        "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\n",
        "\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
        "\n",
        "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \n",
        "    repeat=False)"
      ],
      "metadata": {
        "id": "QL2LOrSR6z7Q",
        "outputId": "720d38c8-d0ff-4f90-de74-91c673bded9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(train_iter)\n",
        "batch=next(it)\n",
        "print(\"The first three text/target sequences from the first batch are:\\n\")\n",
        "indent = \" \" * 4\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()\n",
        " \n",
        "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\n",
        "print(\"Also the sequences continue in the next batch!\\n\")\n",
        "batch = next(it)\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "m85m3IXt7OGP",
        "outputId": "31b0c782-dbac-4767-e263-3cbe16b42222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first three text/target sequences from the first batch are:\n",
            "\n",
            "     Text Sequence 0: <eos> भारतीय मोर या नीला मोर दक्षिण एशिया के देशी तीतर परिवार का एक बड़ा और चमकीले रंग का पक्षी है, दुनिया के अन्य भागों में यह <unk> के रूप में परिचित\n",
            "     Target Sequence 0: भारतीय मोर या नीला मोर दक्षिण एशिया के देशी तीतर परिवार का एक बड़ा और चमकीले रंग का पक्षी है, दुनिया के अन्य भागों में यह <unk> के रूप में परिचित है।\n",
            "\n",
            "     Text Sequence 1: पतला स्तर होता है, परंतु स्त्रियों में <unk> <unk> गर्भाशय गुहा तथा <unk> द्वारा यह बाह्य वातावरण में खुलती है। इस <unk> कला की परतों के द्वारा आशय उदर गुहा में लटके\n",
            "     Target Sequence 1: स्तर होता है, परंतु स्त्रियों में <unk> <unk> गर्भाशय गुहा तथा <unk> द्वारा यह बाह्य वातावरण में खुलती है। इस <unk> कला की परतों के द्वारा आशय उदर गुहा में लटके रहते\n",
            "\n",
            "     Text Sequence 2: चार वर्ष के <unk> पर्यवेक्षित चिकित्सा-कार्य से लेकर तीन से छः वर्षों के डॉक्टर की उपाधि तक होती है, जिसमें नैदानिक स्थापन शामिल होती है। अमेरिका में नैदानिक मनोवैज्ञानिक के लगभग आधे\n",
            "     Target Sequence 2: वर्ष के <unk> पर्यवेक्षित चिकित्सा-कार्य से लेकर तीन से छः वर्षों के डॉक्टर की उपाधि तक होती है, जिसमें नैदानिक स्थापन शामिल होती है। अमेरिका में नैदानिक मनोवैज्ञानिक के लगभग आधे छात्र\n",
            "\n",
            "Each sequence has BPTT_LENGTH = 32.\n",
            "\n",
            "Also the sequences continue in the next batch!\n",
            "\n",
            "     Text Sequence 0: है। नर, मोर, मुख्य रूप से नीले रंग के होते हैं साथ ही इनके पंख पर चपटे चम्मच की तरह नीले रंग की आकृति जिस पर रंगीन आंखों की तरह <unk> बनी\n",
            "     Target Sequence 0: नर, मोर, मुख्य रूप से नीले रंग के होते हैं साथ ही इनके पंख पर चपटे चम्मच की तरह नीले रंग की आकृति जिस पर रंगीन आंखों की तरह <unk> बनी होती\n",
            "\n",
            "     Text Sequence 1: रहते हैं। <eos> इन तंत्रों का वर्णन निम्नलिखित है : <eos> <unk> <unk> <unk> तथा इनकी रुधिर <unk> आदि इस तंत्र के अंतर्गत हैं। वृक्क के दो गोले कटि कशेरुक के दोनों\n",
            "     Target Sequence 1: हैं। <eos> इन तंत्रों का वर्णन निम्नलिखित है : <eos> <unk> <unk> <unk> तथा इनकी रुधिर <unk> आदि इस तंत्र के अंतर्गत हैं। वृक्क के दो गोले कटि कशेरुक के दोनों ओर\n",
            "\n",
            "     Text Sequence 2: छात्र पीएचडी कार्यक्रमों में प्रशिक्षित किए जा रहे हैं- यह एक ऐसा प्रारूप है, जिसमें अनुसंधान पर बल डाला जाता है; अन्य आधे छात्र <unk> कार्यक्रम के होते हैं, जो चिकित्सा-कार्य पर\n",
            "     Target Sequence 2: पीएचडी कार्यक्रमों में प्रशिक्षित किए जा रहे हैं- यह एक ऐसा प्रारूप है, जिसमें अनुसंधान पर बल डाला जाता है; अन्य आधे छात्र <unk> कार्यक्रम के होते हैं, जो चिकित्सा-कार्य पर केंद्रित\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
        "                 dropout=0.5):\n",
        "        ''' Initialize model parameters corresponding to ---\n",
        "            - embedding layer\n",
        "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
        "              optionally more than one layer\n",
        "            - linear layer to map from hidden vector to the vocabulary\n",
        "            - optionally, dropout layers.  Dropout layers can be placed after \n",
        "              the embedding layer or/and after the RNN layer. Dropout within\n",
        "              an RNN is only applied when there are two or more num_layers.\n",
        "            - optionally, initialize the model parameters.\n",
        "            \n",
        "            The arguments are:\n",
        "            \n",
        "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
        "            vocab_size: size of vocabulary\n",
        "            embedding_dim: size of an embedding vector\n",
        "            hidden_dim: size of hidden/state vector in RNN\n",
        "            num_layers: number of layers in RNN\n",
        "            dropout: dropout probability.\n",
        "            \n",
        "        '''\n",
        "        super(RNNLM, self).__init__()\n",
        "        \n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "        # Embedding layer definition: \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # RNN definition - depnding on input type requested for RNN\n",
        "        if rnn_type == 'LSTM':\n",
        "          self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "          self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
        "        elif rnn_type == 'RNN_TANH':\n",
        "          self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, nonlinearity='tanh', dropout=dropout)\n",
        "        elif rnn_type == 'RNN_RELU':\n",
        "          self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, nonlinearity='relu', dropout=dropout)\n",
        "        else:\n",
        "          print(\"Enter one of the four inputs : LSTM/GRU/RNN_TANH/RNN_RELU\")  \n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size) \n",
        "\n",
        "\n",
        "    def forward(self, input, hidden0):\n",
        "        ''' \n",
        "        Run forward propagation for a given minibatch of inputs using\n",
        "        hidden0 as the initial hidden state.\n",
        "\n",
        "        In LSTMs hidden0 = (h_0, c_0). \n",
        "\n",
        "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
        "        Return this as well so that it can be used to initialize the next\n",
        "        batch.\n",
        "        \n",
        "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
        "        the more efficient CrossEntropyLoss.  See \n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
        "        '''\n",
        "\n",
        "        embeds = self.embedding(input)\n",
        "        output_n, hidden_n = self.rnn(embeds, hidden0)\n",
        "        output_n = self.fc(output_n)\n",
        "\n",
        "        return output_n, hidden_n"
      ],
      "metadata": {
        "id": "pZnt4_BG7OIo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):\n",
        "    '''\n",
        "    Evaluate the model on the given data.\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    it = iter(data)\n",
        "    total_count = 0. # Number of target words seen\n",
        "    total_loss = 0. # Loss over all target words\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "        hidden = None \n",
        "        for i, batch in enumerate(it):\n",
        "            ''' Do the following:\n",
        "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
        "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "                  the current batch. \n",
        "                - Call forward propagation to get output and final hidden state vector.\n",
        "                - Compute the cross entropy loss\n",
        "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
        "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
        "                  total count (of target words) and total loss see so far over all batches.\n",
        "            '''\n",
        "            text, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                text, target = text.cuda(), target.cuda()\n",
        "            output, hidden = model(text, hidden)\n",
        "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                  \n",
        "            total_count += np.multiply(*text.size())\n",
        "            total_loss += loss.item()*np.multiply(*text.size())\n",
        "                \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4R5Km6mq7OK-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 3\n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if h is None:\n",
        "        return None\n",
        "    elif isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "''' Do the following:\n",
        "    - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "      the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "      https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda -DONE\n",
        "    - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "      the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
        "      the provided repackage_hidden(). See\n",
        "      https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
        "    - Zero out the model gradients to reset backpropagation for current batch\n",
        "    - Call forward propagation to get output and final hidden state vector.\n",
        "    - Compute the cross entropy loss\n",
        "    - Run back propagation to set the gradients for each model parameter.\n",
        "    - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
        "      https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
        "    - Run a step of gradient descent. \n",
        "    - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
        "    - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
        "      your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
        "      copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
        "      https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
        "      in Sec 2.3.1 of Lecture notes by Cho: \n",
        "      https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
        "'''\n",
        "\n",
        "\n",
        "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "print(\"model:\", model)\n",
        "print(\"use_cuda_status: \", USE_CUDA)\n",
        "if USE_CUDA:\n",
        "  model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "best_model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    log_interval = 1000\n",
        "    it = iter(train_iter)\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "\n",
        "    for i, batch in enumerate(it):\n",
        "      model.zero_grad()\n",
        "      text, target = batch.text, batch.target\n",
        "      if USE_CUDA:\n",
        "        text, target = text.cuda(), target.cuda()\n",
        "      output_n, hidden_n = model(text, hidden)\n",
        "      hidden = repackage_hidden(hidden_n)\n",
        "      loss = loss_fn(output_n.view(-1, output_n.size(-1)), target.view(-1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2.0, error_if_nonfinite=False)\n",
        "\n",
        "      if i % log_interval == 0 and i > 0:\n",
        "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
        "\n",
        "      if i % 2000 == 0 and i > 0:\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "        print(f'Iteration {i}, the validation loss is {val_loss:.3f}.')\n",
        "        val_losses.append(val_loss)\n",
        "        if val_loss <= min(val_losses):\n",
        "          torch.save(model.state_dict(), \"gdrive/My Drive/AML_Project/best_model.pt\")\n",
        "\n",
        "best_model.load_state_dict(torch.load(\"gdrive/My Drive/AML_Project/best_model.pt\")) "
      ],
      "metadata": {
        "id": "b5EIvXYS7ONq",
        "outputId": "a966141a-669b-419f-caa2-937ed7ef5f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: RNNLM(\n",
            "  (embedding): Embedding(50002, 650)\n",
            "  (rnn): LSTM(650, 650, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=650, out_features=50002, bias=True)\n",
            ")\n",
            "use_cuda_status:  True\n",
            "At iteration 1000 the loss is 6.524.\n",
            "At iteration 2000 the loss is 6.092.\n",
            "Iteration 2000, the validation loss is 6.088.\n",
            "At iteration 3000 the loss is 5.893.\n",
            "At iteration 4000 the loss is 5.484.\n",
            "Iteration 4000, the validation loss is 5.728.\n",
            "At iteration 5000 the loss is 5.779.\n",
            "At iteration 1000 the loss is 5.283.\n",
            "At iteration 2000 the loss is 5.282.\n",
            "Iteration 2000, the validation loss is 5.522.\n",
            "At iteration 3000 the loss is 5.198.\n",
            "At iteration 4000 the loss is 4.797.\n",
            "Iteration 4000, the validation loss is 5.392.\n",
            "At iteration 5000 the loss is 5.117.\n",
            "At iteration 1000 the loss is 4.678.\n",
            "At iteration 2000 the loss is 4.820.\n",
            "Iteration 2000, the validation loss is 5.478.\n",
            "At iteration 3000 the loss is 4.809.\n",
            "At iteration 4000 the loss is 4.391.\n",
            "Iteration 4000, the validation loss is 5.351.\n",
            "At iteration 5000 the loss is 4.692.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "if USE_CUDA:\n",
        "  best_model = best_model.cuda()\n",
        "val_loss = evaluate(best_model, val_iter)\n",
        "print(\"perplexity: \", np.exp(val_loss))"
      ],
      "metadata": {
        "id": "lzfY3BA-8JF0",
        "outputId": "a6a1dff4-1342-48d3-e034-d42faaa52d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  210.81116793857507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model, test_iter)\n",
        "print(\"perplexity: \", np.exp(test_loss))"
      ],
      "metadata": {
        "id": "-61pscNZ8JIH",
        "outputId": "ddf78336-911e-4421-a733-6d43f7dd8b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  205.139284079365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence generation in Hindi\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# predict next token\n",
        "def predict(best_model, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[TEXT.vocab.stoi[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = repackage_hidden(h)\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = best_model(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out.view(-1, out.size(-1)) , dim=1)\n",
        "\n",
        "  # get indices of top 3 or 5 values\n",
        "  top_n_idx = p.argsort().reshape(-1)[-3:]\n",
        "  #top_n_idx = p.argsort()[-1][-3:]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  #print(sampled_token_index)\n",
        "\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return TEXT.vocab.itos[sampled_token_index], h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(best_model, size, prime='it is'):\n",
        "        \n",
        "    # push to GPU\n",
        "    #best_model.cuda()\n",
        "    \n",
        "    best_model.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = None\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(best_model, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(best_model, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "\n",
        "words_list = [\"सिनेमा\", \"प्राचीन\", \"मलयालम\", \"शरीर\", \"भौतिक\", \"प्रयोग\", \"एक\"]\n",
        "for i in words_list:\n",
        "  print(i,\" :\", sample(best_model, 50, prime=i))"
      ],
      "metadata": {
        "id": "bBNti1wu8JKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b69f45f-d47d-427c-cb22-458b0c77e68c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "सिनेमा  : सिनेमा <unk> <unk> <unk> के नाम के रूप में, <unk> <unk> <unk> और <unk> में <unk> के <unk> <unk> के <unk> के साथ ही साथ <unk> <unk> और अनुनासिकता का उपयोग किया गया था, जिसे उन्होंने एक <unk> में <unk> के रूप में, <unk> की रचना के लिए <unk> <unk> की\n",
            "प्राचीन  : प्राचीन <unk> <unk> <unk> और <unk> <eos> <eos> <eos> <unk> <unk> <unk> <eos> <eos> <unk> improve <unk> · difference of china of conservation · south <unk> <unk> <eos> <unk> <unk> · <unk> <unk> <eos> <unk> · difference <eos> m: pso/psi <eos> <unk> <unk> · more... · republic · south ossetia <unk>\n",
            "मलयालम  : मलयालम <unk> <unk> <unk> <unk> <eos> <eos> अम्मा के एक <unk> में से कुछ <unk> का उपयोग करके यह एक महत्वपूर्ण कवयित्री था। इस प्रकार की <unk> के कारण <unk> <unk> के नाम के रूप में, यह एक महत्वपूर्ण कवयित्री है, जिसे उन्होंने <unk> कहा है। इस बात की पुष्टि करते\n",
            "शरीर  : शरीर का उपयोग करके <unk> के <unk> <unk> के साथ एक साथ <unk> <unk> के रूप में, यह एक विशेष प्रकार की <unk> <unk> <unk> <unk> <unk> और <unk> में <unk> और <unk> के बीच की तुलना की गई है, जो <unk> के <unk> से <unk> की अतिवृद्धि को कम करती\n",
            "भौतिक  : भौतिक <unk> <unk> और अनुनासिकता <unk> <unk> <unk> और अन्य प्रमुख कशेरुकी क्षेत्र: <unk> के रूप में, <unk> और <unk> के साथ मिलकर <unk> <unk> के रूप में। इस प्रकार के लोग भी शामिल हैं जैसे <unk> <unk> और अन्य <unk> जैसे कि <unk> <unk> और अनुनासिकता के <unk> <unk> <unk>\n",
            "प्रयोग  : प्रयोग और <unk> में <unk> के लिए किया गया था। <unk> के <unk> <unk> <unk> और <unk> के साथ <unk> <unk> और अन्य कई प्रकार की अच्छी-अच्छी कवयित्रियों में <unk> <unk> <unk> <unk> <unk> और रुक्मिणी के साथ एक <unk> के लिए <unk> की रचना के साथ ही साथ साथ साथ\n",
            "एक  : एक दूसरे प्रकार से प्राप्त होने वाले पहले ही इन्हे एक दूसरे से <unk> के लिए इस्तेमाल किए जाने के बाद भी यह एक महत्वपूर्ण समस्या है और इसके लिए <unk> के रूप में। इस परियोजना को यह भी पता चलता है कि, इस बात की संभावना थी कि वे अपने\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen1 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार एक\"\n",
        "        \"छोटे कुत्ते के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen2 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"समुद्र के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen3 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"एक पेंसिल के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen4 = (\"डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार क्यूबा बराबर होता है।\"\n",
        "         \"प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['डैविल एक मांसाहारी धानीप्राणी है']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()"
      ],
      "metadata": {
        "id": "FpxKIiks8JNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611f1e46-c1e2-44da-a793-1bcacb91248c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार एकछोटे कुत्ते के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  16106.210474284047\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारसमुद्र के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  15410.408300338655\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारएक पेंसिल के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  10440.019782252652\n",
            "\n",
            "डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार क्यूबा बराबर होता है।प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  15992.96127426009\n",
            "\n",
            "बन है।इसका का होता जाता के एकछोटे ऑस्ट्रेलिया के बड़ा एक के है।1936 जंगलों बराबर अब दुनिया बाद में यह में है पाया सबसे गया। राज्य आकार होने धानीप्राणी तस्मानियाई जो केवल के ही तस्मानिया थायलेसीन​ कुत्ते मांसाहारी धानीप्राणी द्वीप विलुप्त डैविल के मांसाहारी\n",
            "perplexity:  71163.8992834149\n",
            "\n",
            "डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है\n",
            "perplexity:  78986.39842766778\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(i) Substitute LSTM with a GRU\n",
        "\n",
        "model_GRU = RNNLM(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "print(\"model_GRU:\", model_GRU)\n",
        "print(\"use_cuda_status: \", USE_CUDA)\n",
        "if USE_CUDA:\n",
        "  model_GRU = model_GRU.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model_GRU.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "best_model_GRU = RNNLM(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model_GRU.train()\n",
        "    log_interval = 1000\n",
        "    it = iter(train_iter)\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "\n",
        "    for i, batch in enumerate(it):\n",
        "      model_GRU.zero_grad()\n",
        "      text, target = batch.text, batch.target\n",
        "      if USE_CUDA:\n",
        "        text, target = text.cuda(), target.cuda()\n",
        "      output_n, hidden_n = model_GRU(text, hidden)\n",
        "      hidden = repackage_hidden(hidden_n)\n",
        "      loss = loss_fn(output_n.view(-1, output_n.size(-1)), target.view(-1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      nn.utils.clip_grad_norm_(model_GRU.parameters(), max_norm=GRAD_CLIP, norm_type=2.0, error_if_nonfinite=False)\n",
        "\n",
        "      if i % log_interval == 0 and i > 0:\n",
        "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
        "\n",
        "      if i % 2000 == 0 and i > 0:\n",
        "        val_loss = evaluate(model_GRU, val_iter)\n",
        "        print(f'Iteration {i}, the validation loss is {val_loss:.3f}.')\n",
        "        val_losses.append(val_loss)\n",
        "        if val_loss <= min(val_losses):\n",
        "          torch.save(model_GRU.state_dict(), \"gdrive/My Drive/AML_Project/best_model_GRU.pt\")\n",
        "\n",
        "best_model_GRU.load_state_dict(torch.load(\"gdrive/My Drive/AML_Project/best_model_GRU.pt\")) "
      ],
      "metadata": {
        "id": "wYAp3Hez8JPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc0e253-06a2-4066-f97f-613df114b366"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_GRU: RNNLM(\n",
            "  (embedding): Embedding(50002, 650)\n",
            "  (rnn): GRU(650, 650, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=650, out_features=50002, bias=True)\n",
            ")\n",
            "use_cuda_status:  True\n",
            "At iteration 1000 the loss is 7.806.\n",
            "At iteration 2000 the loss is 7.722.\n",
            "Iteration 2000, the validation loss is 7.714.\n",
            "At iteration 3000 the loss is 7.492.\n",
            "At iteration 4000 the loss is 7.517.\n",
            "Iteration 4000, the validation loss is 7.644.\n",
            "At iteration 5000 the loss is 7.649.\n",
            "At iteration 1000 the loss is 6.857.\n",
            "At iteration 2000 the loss is 5.979.\n",
            "Iteration 2000, the validation loss is 6.124.\n",
            "At iteration 3000 the loss is 5.789.\n",
            "At iteration 4000 the loss is 5.285.\n",
            "Iteration 4000, the validation loss is 5.674.\n",
            "At iteration 5000 the loss is 5.680.\n",
            "At iteration 1000 the loss is 5.335.\n",
            "At iteration 2000 the loss is 5.324.\n",
            "Iteration 2000, the validation loss is 5.510.\n",
            "At iteration 3000 the loss is 5.299.\n",
            "At iteration 4000 the loss is 4.803.\n",
            "Iteration 4000, the validation loss is 5.462.\n",
            "At iteration 5000 the loss is 5.320.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity score on validation and test datasets\n",
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "if USE_CUDA:\n",
        "  best_model_GRU = best_model_GRU.cuda()\n",
        "val_loss = evaluate(best_model_GRU, val_iter)\n",
        "print(\"perplexity on validation set: \", np.exp(val_loss))\n",
        "\n",
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model_GRU, test_iter)\n",
        "print(\"perplexity on test set: \", np.exp(test_loss))"
      ],
      "metadata": {
        "id": "O86dXrtL8JRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3279b6b-7a30-459b-d494-3d6fa99201b4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity on validation set:  235.55341061618014\n",
            "perplexity on test set:  234.98294424452448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence generation in Hindi using GRU model\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# predict next token\n",
        "def predict(best_model_GRU, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[TEXT.vocab.stoi[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = repackage_hidden(h)\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = best_model(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out.view(-1, out.size(-1)) , dim=1)\n",
        "\n",
        "  # get indices of top 3 or 5 values\n",
        "  top_n_idx = p.argsort().reshape(-1)[-3:]\n",
        "  #top_n_idx = p.argsort()[-1][-3:]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  #print(sampled_token_index)\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return TEXT.vocab.itos[sampled_token_index], h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(model, size, prime='it is'):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = None\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(best_model, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(best_model, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "\n",
        "words_list = [\"सिनेमा\", \"प्राचीन\", \"मलयालम\", \"शरीर\", \"भौतिक\", \"प्रयोग\", \"एक\"]\n",
        "for i in words_list:\n",
        "  print(i,\" :\", sample(best_model_GRU, 50, prime=i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDyXVTz_Fczv",
        "outputId": "6ccc64d6-de00-4db4-f5a8-97e66c92ee99"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "सिनेमा  : सिनेमा में एक <unk> के लिए एक महत्वपूर्ण भूमिका निभाते थे। इस परियोजना ने इस परियोजना को बढ़ावा देने के लिए इस बात का समर्थन किया है और इस बात को देखते हैं और यह एक बार जब <unk> की <unk> के साथ ही साथ साथ एक साथ दिया जा सकता\n",
            "प्राचीन  : प्राचीन <unk> के रूप मे भी किया गया है। <eos> इस बात पर सहमत है और यह भी देखें: <eos> <unk> के <unk> <unk> <unk> के <unk> में स्थित एक एक प्रमुख <unk> के साथ <unk> <unk> के साथ एक <unk> और <unk> के रूप में, एक नया रूप <unk> <unk>\n",
            "मलयालम  : मलयालम में एक <unk> <unk> का प्रयोग <unk> में <unk> और <unk> <unk> की रचना करने वाले एक नए स्थान के साथ <unk> की रचना के रूप में <unk> <unk> के लिए एक विशेष प्रकार के संयुक्‍ताक्षर के साथ <unk> <unk> के रूप में वर्णित किया जा रहा है। यह संयुक्‍त\n",
            "शरीर  : शरीर के लिए किया जा सकता है और <unk> में <unk> <unk> और अन्य सभी प्रकार का उपयोग किया गया है और वे भी <unk> की ओर ले जाती हैं और वे एक दूसरे के साथ एक साथ काम करने की अनुमति देते हैं, जैसे :– कहाँ, और रचनावाद, जैसे <unk>\n",
            "भौतिक  : भौतिक <unk> के <unk> <unk> <unk> के साथ ही साथ काम करना शुरू कर दिया। <eos> <eos> <eos> <unk> <unk> <eos> <unk> <eos> <unk> के <unk> के <unk> के <unk> के लिए एक नया ग्रीनफील्ड रूप त्र के रूप में, यह एक महत्वपूर्ण समस्या है जो एक ही <unk> के साथ\n",
            "प्रयोग  : प्रयोग किया जा रहा है और इस तरह की <unk> <unk> की एक छोटी संख्या की वजह से। <eos> <unk> के अनुसार एक <unk> <unk> <unk> और एक ही एक ही समय का उपयोग किया गया है, जो एक बार में <unk> <unk> के साथ एक <unk> का एक भाग है,\n",
            "एक  : एक विशेष प्रकार से भी <unk> की <unk> <unk> <unk> और अनुनासिकता ऑफ़ द वर्ल्ड्स के साथ काम किया गया था, लेकिन वे <unk> की सहायता के बाद से एक और अधिक <unk> <unk> की <unk> के कारण उनकी <unk> के कारण यह भी कहा जा सकता है और <unk> की\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity scoring for the 5 setences using GRU model\n",
        "\n",
        "sen1 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार एक\"\n",
        "        \"छोटे कुत्ते के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen2 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"समुद्र के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen3 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"एक पेंसिल के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen4 = (\"डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार क्यूबा बराबर होता है।\"\n",
        "         \"प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['डैविल एक मांसाहारी धानीप्राणी है']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model_GRU, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTkAQFIgFc2r",
        "outputId": "829f78da-45e0-4cf6-f6c6-7b3e6a7b4016"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार एकछोटे कुत्ते के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  2255.8927662471897\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारसमुद्र के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  1774.3572834034035\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारएक पेंसिल के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  1664.1740563314036\n",
            "\n",
            "डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार क्यूबा बराबर होता है।प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  2561.2150087119458\n",
            "\n",
            "गया। तस्मानियाई में धानीप्राणी जंगलों सबसे ही बराबर मांसाहारी अब होने ऑस्ट्रेलिया बाद राज्य के का थायलेसीन​ मांसाहारी एक जाता होता है है।1936 कुत्ते के बन आकार दुनिया तस्मानिया के पाया है।इसका द्वीप के धानीप्राणी जो केवल बड़ा के एकछोटे डैविल यह विलुप्त में\n",
            "perplexity:  9914.972823963491\n",
            "\n",
            "डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है\n",
            "perplexity:  9581.780901286456\n",
            "\n"
          ]
        }
      ]
    }
  ]
}